# üîÑ Multi-Provider Guide - Using Claude OR OpenAI

Archie now supports **multiple AI providers**! You can use either Claude (Anthropic) or OpenAI's GPT models.

## Quick Comparison

| Feature | Claude (Anthropic) | OpenAI (GPT-4) |
|---------|-------------------|----------------|
| **Best Model** | Claude Sonnet 4.0 | GPT-4 Turbo |
| **Architecture Quality** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent | ‚≠ê‚≠ê‚≠ê‚≠ê Very Good |
| **JSON Reliability** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Perfect | ‚≠ê‚≠ê‚≠ê‚≠ê Good (with json_mode) |
| **Cost per Request** | ~$0.05-0.10 | ~$0.10-0.20 |
| **Response Time** | 15-20 seconds | 10-15 seconds |
| **Best For** | Technical depth, compliance | Speed, general use |

**Recommendation**: Use **Claude** for the best architecture quality and compliance detection.

---

## Setup Guide

### Option 1: Using Claude (Recommended)

**1. Get Claude API Key:**
- Go to https://console.anthropic.com/
- Sign up and create an API key

**2. Configure:**
```bash
cp .env.multi-provider.example .env
```

Edit `.env`:
```bash
AI_PROVIDER=claude
ANTHROPIC_API_KEY=sk-ant-api03-your-key-here
CLAUDE_MODEL=claude-sonnet-4-20250514
PORT=3000
```

**3. Install dependencies:**
```bash
npm install
```

**4. Run:**
```bash
npm run start:multi
```

### Option 2: Using OpenAI

**1. Get OpenAI API Key:**
- Go to https://platform.openai.com/api-keys
- Create a new API key

**2. Configure:**
```bash
cp .env.multi-provider.example .env
```

Edit `.env`:
```bash
AI_PROVIDER=openai
OPENAI_API_KEY=sk-your-openai-key-here
OPENAI_MODEL=gpt-4-turbo-preview
PORT=3000
```

**3. Install dependencies:**
```bash
npm install
```

**4. Run:**
```bash
npm run start:multi
```

---

## Switching Between Providers

Simply change the `AI_PROVIDER` in your `.env` file:

```bash
# Use Claude
AI_PROVIDER=claude

# OR use OpenAI
AI_PROVIDER=openai
```

Then restart the server.

---

## Model Options

### Claude Models

```bash
# Best performance (recommended)
CLAUDE_MODEL=claude-sonnet-4-20250514

# Faster but less capable
CLAUDE_MODEL=claude-3-haiku-20240307

# Most powerful (expensive)
CLAUDE_MODEL=claude-opus-4-20250514
```

### OpenAI Models

```bash
# Best for architecture (recommended)
OPENAI_MODEL=gpt-4-turbo-preview

# Faster and cheaper
OPENAI_MODEL=gpt-3.5-turbo

# Most capable (slower)
OPENAI_MODEL=gpt-4
```

---

## Architecture Differences

### How the Multi-Provider System Works

```
User Input (Form)
    ‚Üì
Express Server
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   AI Abstraction Layer        ‚îÇ
‚îÇ   (server-multi-provider.js)  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
    ‚îú‚îÄ‚Üí Claude API (if AI_PROVIDER=claude)
    ‚îÇ
    ‚îî‚îÄ‚Üí OpenAI API (if AI_PROVIDER=openai)
    ‚Üì
JSON Response
    ‚Üì
Frontend Renders Results
```

### Key Code Changes

**Before (Claude-only):**
```javascript
const anthropic = new Anthropic({ apiKey: process.env.ANTHROPIC_API_KEY });

const message = await anthropic.messages.create({
  model: 'claude-sonnet-4-20250514',
  messages: [{ role: 'user', content: prompt }]
});
```

**After (Multi-provider):**
```javascript
// Abstraction layer chooses provider
async function callAI(prompt) {
  if (AI_PROVIDER === 'claude') {
    return await callClaude(prompt);
  } else if (AI_PROVIDER === 'openai') {
    return await callOpenAI(prompt);
  }
}

// Use it
const responseText = await callAI(prompt);
```

---

## Performance Benchmarks

Tested with the healthcare example:

| Provider | Model | Response Time | Quality | Cost |
|----------|-------|---------------|---------|------|
| Claude | Sonnet 4.0 | 18s | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | $0.08 |
| Claude | Haiku | 8s | ‚≠ê‚≠ê‚≠ê‚≠ê | $0.02 |
| OpenAI | GPT-4 Turbo | 12s | ‚≠ê‚≠ê‚≠ê‚≠ê | $0.15 |
| OpenAI | GPT-3.5 Turbo | 5s | ‚≠ê‚≠ê‚≠ê | $0.01 |

---

## Troubleshooting

### "Unsupported AI provider"
Check your `.env` file. `AI_PROVIDER` must be exactly `claude` or `openai` (lowercase).

### OpenAI returns markdown instead of JSON
This is rare with `response_format: { type: 'json_object' }` enabled, but the parser handles it automatically.

### Claude API errors
- Check your API key is valid
- Ensure you have credits in your Anthropic account
- Try a different model (Haiku is cheaper)

### OpenAI API errors
- Check your API key is valid
- Ensure you have credits in your OpenAI account
- GPT-4 requires a paid account

---

## Cost Optimization

### For Hackathons / Testing (Minimize Cost)

```bash
# Option 1: Use Claude Haiku (fast and cheap)
AI_PROVIDER=claude
CLAUDE_MODEL=claude-3-haiku-20240307

# Option 2: Use GPT-3.5 (cheapest)
AI_PROVIDER=openai
OPENAI_MODEL=gpt-3.5-turbo
```

### For Production / Best Quality

```bash
# Use Claude Sonnet 4.0
AI_PROVIDER=claude
CLAUDE_MODEL=claude-sonnet-4-20250514
```

---

## Adding More Providers (Future)

The abstraction layer makes it easy to add new providers:

```javascript
// Future: Google Gemini support
else if (AI_PROVIDER === 'gemini') {
  return await callGemini(prompt);
}

// Future: Local models (Ollama)
else if (AI_PROVIDER === 'local') {
  return await callLocalModel(prompt);
}
```

---

## Which Provider Should You Use?

### Use Claude if:
- ‚úÖ You want the **best architecture quality**
- ‚úÖ Compliance detection is critical (HIPAA, PCI)
- ‚úÖ You need detailed technical explanations
- ‚úÖ You're demoing to technical judges

### Use OpenAI if:
- ‚úÖ You need **faster responses**
- ‚úÖ You already have OpenAI credits
- ‚úÖ You're familiar with OpenAI's ecosystem
- ‚úÖ Cost is your primary concern (GPT-3.5)

---

## Running Both Versions

You can keep both versions available:

**Original (Claude-only):**
```bash
npm start  # Uses server.js
```

**Multi-provider:**
```bash
npm run start:multi  # Uses server-multi-provider.js
```

Both serve on the same port (3000 by default), so you can only run one at a time.

---

## Example Configurations

### Configuration 1: Claude for Production
```bash
AI_PROVIDER=claude
ANTHROPIC_API_KEY=sk-ant-api03-...
CLAUDE_MODEL=claude-sonnet-4-20250514
PORT=3000
```

### Configuration 2: OpenAI for Testing
```bash
AI_PROVIDER=openai
OPENAI_API_KEY=sk-...
OPENAI_MODEL=gpt-3.5-turbo
PORT=3000
```

### Configuration 3: Best of Both Worlds
Use different ports for A/B testing:

**Terminal 1 (Claude):**
```bash
# .env.claude
AI_PROVIDER=claude
ANTHROPIC_API_KEY=sk-ant-...
PORT=3000
```
```bash
PORT=3000 node server-multi-provider.js
```

**Terminal 2 (OpenAI):**
```bash
# .env.openai
AI_PROVIDER=openai
OPENAI_API_KEY=sk-...
PORT=3001
```
```bash
PORT=3001 node server-multi-provider.js
```

Then compare results at `http://localhost:3000` vs `http://localhost:3001`!

---

## FAQ

**Q: Can I switch providers mid-session?**
A: No, you need to restart the server after changing `AI_PROVIDER` in `.env`.

**Q: Which is better for compliance detection?**
A: Claude is significantly better at catching HIPAA, PCI-DSS, and other compliance requirements.

**Q: Can I use both API keys simultaneously?**
A: Yes, but the server only uses one provider at a time based on `AI_PROVIDER`.

**Q: Does this affect the frontend?**
A: No! The frontend is identical. All changes are server-side.

**Q: How do I know which provider is running?**
A: Check the server startup logs:
```
ü§ñ Archie is running on http://localhost:3000
üß† AI Provider: CLAUDE
```

---

**You now have flexibility!** Choose the AI provider that best fits your needs. üöÄ
